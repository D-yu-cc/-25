# 项目周报

日期：2025-04-07

项目实践题目：文本的匹配与推荐

## 实践内容

### 文献阅读

PMI值：正值为共现显著，负值为共现不足（常不可靠）。
因此引进PPMI，将负PMI值替换为0，避免噪声干扰。

word2vec（词嵌入模型）：

首先，word2vec 简化了任务(使其成为二进制分类而不是单词预测)。其次，word2vec 简化了体系结构(训练逻辑回归分类器，而不是使用需要更复杂训练算法的具有隐藏层的多层神经网络)

6.8.1 主要讲了Skip-gram模型 
目标：训练一个二分类器，预测 $c$ 是否是 $w$ 的真实上下文词，即计算概率 $P(+|w,c)$（目标词 $w$ 和候选上下文词 $c$）。
相似度计算：通过词嵌入的点积衡量相似度：
$$
  \text{Similarity}(w,c) \propto \mathbf{c} \cdot \mathbf{w}
$$
  
使用sigmoid函数将点积转换为概率：
$$
P(+|w,c) = \sigma(\mathbf{c} \cdot \mathbf{w}) = \frac{1}{1+\exp(-\mathbf{c} \cdot \mathbf{w})}
$$

独立性假设：

假设上下文窗口内的所有上下文词彼此独立，整体概率为各词概率的乘积：
$$
P(+|w,c_{1:L}) = \prod_{i=1}^L \sigma(\mathbf{c}_i \cdot \mathbf{w})
$$

对数概率形式便于优化：
$$
\log P(+|w,c_{1:L}) = \sum_{i=1}^L \log \sigma(\mathbf{c}_i \cdot \mathbf{w})
$$

6.8.2讲了参数与训练

嵌入矩阵
双矩阵结构：每个词有两种嵌入：
目标嵌入矩阵 \(W\)：词作为目标时的向量（如"apricot"）。
上下文嵌入矩阵 \(C\)：词作为上下文时的向量（如"jam"）。
最终表示：通常将两者相加 \( \mathbf{w}_i + \mathbf{c}_i \)，或仅使用目标矩阵 \(W\)。

损失函数与优化

损失函数：结合正样本（真实上下文词）和负样本（噪声词）的对数概率：
\[
\mathcal{L} = -\left[ \log \sigma(\mathbf{c}_{\text{pos}} \cdot \mathbf{w}) + \sum_{i=1}^k \log \sigma(-\mathbf{c}_{\text{neg}_i} \cdot \mathbf{w}) \right]
\]

正样本：最大化相似度（如"apricot"与"jam"）。
负样本（\(k\)个随机噪声词）：最小化相似度（如"apricot"与"aardvark"）。
  
梯度下降更新**：
对目标词和上下文词向量的梯度计算：
\[
\frac{\partial \mathcal{L}}{\partial \mathbf{c}_{\text{pos}}} = [\sigma(\mathbf{c}_{\text{pos}} \cdot \mathbf{w}) - 1] \mathbf{w}
\]
\[
\frac{\partial \mathcal{L}}{\partial \mathbf{w}} = [\sigma(\mathbf{c}_{\text{pos}} \cdot \mathbf{w}) - 1] \mathbf{c}_{\text{pos}} + \sum_{i=1}^k [\sigma(\mathbf{c}_{\text{neg}_i} \cdot \mathbf{w})] \mathbf{c}_{\text{neg}_i}
\]
参数更新公式：
\[
\mathbf{c}_{\text{pos}}^{t+1} = \mathbf{c}_{\text{pos}}^t - \eta [\sigma(\mathbf{c}_{\text{pos}} \cdot \mathbf{w}) - 1] \mathbf{w}
\]
\[
\mathbf{w}^{t+1} = \mathbf{w}^t - \eta \left[ [\sigma(\mathbf{c}_{\text{pos}} \cdot \mathbf{w}) - 1] \mathbf{c}_{\text{pos}} + \sum_{i=1}^k \sigma(\mathbf{c}_{\text{neg}_i} \cdot \mathbf{w}) \mathbf{c}_{\text{neg}_i} \right]
\]

负采样策略
噪声词选择：根据加权的词频分布 \( P_\alpha(w) \) 采样（通常 \(\alpha=0.75\)）：
\[
P_\alpha(w) \propto \text{count}(w)^\alpha
\]
提升低频词被选为噪声的概率，避免高频词主导。

这周阅读文献速度加快，用AI先提取总结一下主要内容，以此为大纲，去有选择地着重阅读，抄写文档中的公式，练习公式的输入，熟练度有所提高，但还有很大的进步空间。

### 其他

支线任务：进入官网后，先试着搜索了一下以“LLM”为关键词的论文，可以看到被引用次数，所有版本数等。

在下周应继续加快文献的阅读，同时多熟悉一下公式的输入。