# 项目总结

## 问题描述

项目论文主要解决了在零样本的情况下，如何构建高效的密集检索（dense retrieval）系统的问题。具体来说，论文提出了HyDE（Hypothetical Document Embeddings）通过生成假设文档，再用无监督编码器检索真实文档；通过查资料，了解到GPL（Generative Pseudo Labeling）：结合生成模型和对比学习，提升零样本检索。两者都是利用生成模型自动生成伪相关数据，再训练检索模型，当然也有不同之处。

### HyDE（Hypothetical Document Embeddings）

HyDE通过生成"假设文档"作为查询与真实文档库之间的桥梁，将查询-文档相似度计算分解为两个子任务：
1.生成任务：用指令微调的大模型（如InstructGPT）生成与查询相关的虚构文档。
2.检索任务：用无监督对比编码器将假设文档映射到真实文档的嵌入空间进行检索。

#### 数学框架

**1. 生成假设文档**

$$
\hat{d} \sim g(q, \text{INST})
$$
其中：
- $g$：指令遵循的语言模型（如InstructGPT）
- $q$：输入查询
- INST：生成指令

**2. 编码假设文档**

$$
\mathbf{v}_{\hat{d}} = f(\hat{d})
$$
其中：
- $f$：对比学习编码器（如Contriever）
- $\mathbf{v}_{\hat{d}}$：假设文档的嵌入向量

**3. 计算相似度**
 
$$
\text{sim}(q, d) = \langle \mathbf{v}_{\hat{d}}, \mathbf{v}_{d} \rangle
$$
其中：
- $\mathbf{v}_{d}$：真实文档$d$的嵌入向量
- $\langle \cdot, \cdot \rangle$：向量内积

代表论文：Precise Zero-Shot Dense Retrieval without Relevance Labels（项目论文）

### GPL（Generative Pseudo Labeling）

GPL 是一种用于无监督域适应的稠密检索方法。它利用生成式模型（如T5、GPT-3）自动生成伪相关标签，从而在目标域上训练稠密检索模型，而无需人工标注数据。
**核心思想：**

1.生成伪查询-文档对：使用生成模型为未标注的目标域文档自动生成相关查询。
2.负采样：从目标域中采样负例文档，构建对比学习任务。
3.训练稠密检索模型：利用生成的伪标签进行监督训练，使模型适应目标域

#### 核心公式

**1. 生成伪查询（Pseudo Query Generation）**
给定目标域文档 \( d \)，使用生成模型 \( G \) 生成相关查询 \( q \)：
$$
q \sim G(d)
$$

**2. 构建三元组（Query-Positive-Negative Triplet）**
- **正例**：生成的查询 \( q \) 及其对应文档 \( d^+ = d \)。
- **负例**：从目标域随机采样不相关文档 \( d^- \)。
- 三元组形式：
  \[
  \mathcal{T} = (q, d^+, d^-)
  \]

**3. 对比学习损失（Contrastive Loss）**
使用 **InfoNCE** 损失优化稠密检索模型 \( f_\theta \)：
\[
\mathcal{L}_{\text{GPL}} = -\log \frac{e^{f_\theta(q)^\top f_\theta(d^+)} }{ e^{f_\theta(q)^\top f_\theta(d^+)} + \sum_{i=1}^K e^{f_\theta(q)^\top f_\theta(d_i^-)} }
\]
其中：
- \( f_\theta(q) \) 和 \( f_\theta(d) \) 分别是查询和文档的稠密向量表示。
- \( K \) 是负例数量（通常采用 in-batch negatives）。

**4. 最终训练目标**
结合生成伪标签和对比学习，优化检索模型：
\[
\theta^* = \arg\min_\theta \mathbb{E}_{(q,d^+,d^-) \sim \mathcal{D}_{\text{pseudo}}} \left[ \mathcal{L}_{\text{GPL}} \right]
\]
其中 \( \mathcal{D}_{\text{pseudo}} \) 是生成的伪标签数据集。

代表论文：GPL: Generative Pseudo Labeling for Unsupervised Domain Adaptation of Dense Retrieval（提出 GPL 方法，结合生成式模型（如 T5）和交叉编码器（Cross-Encoder）为目标域生成伪标签，用于无监督领域适应的稠密检索训练）

### 两者对比

#### 关键区别

1. **是否需要训练**
   - HyDE：完全零样本方法，直接使用预训练的生成模型（如InstructGPT）和检索编码器（如Contriever），无需任何训练或微调步骤。
   - GPL：需要两阶段训练，首先生成模型（如T5）为未标注数据生成伪查询，然后使用这些伪标签对稠密检索模型（如DPR）进行监督式微调

2. **适用场景**
  - HyDE：适用于零样本(Zero-shot)和冷启动(Cold Start)场景，当没有标注数据或无法获取领域数据时，可直接部署使用。典型场景包括：新业务快速上线、低资源语言检索、突发事件的即时搜索需求等。
  - GPL：适用于需要对特定领域进行优化的场景，当拥有目标领域文档（即使未标注）时，可通过生成伪标签实现领域自适应(Domain Adaptation)。典型场景包括：金融/医疗等专业领域搜索、企业知识库定制化搜索等。

**选择建议**
  - 需要立即使用 ➔ 选择HyDE：适合PoC验证、临时性需求或资源受限场景
  - 有目标领域数据 ➔ 选择GPL：适合长期运营的专业搜索系统
  - 两者可结合使用：先用HyDE快速搭建原型，积累数据后切换至GPL优化