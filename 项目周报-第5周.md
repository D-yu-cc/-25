# 项目周报

日期：2025-05-01

项目实践题目：文本的匹配与推荐

## 实践内容

### 文献阅读

​​Transformer架构​​：
文档中提到的编码器-解码器模型（如RNN/Transformer）是LLM的基础架构。例如：
​​编码器​​：BERT等模型使用编码器结构，学习上下文表示。
​​解码器​​：GPT系列模型基于自回归解码器生成文本。
​​注意力机制​​：
​​自注意力（Self-Attention）​​：Transformer的核心机制，允许模型动态关注输入序列的不同部分，解决长距离依赖问题。
​​动态上下文​​：解码器的每一步通过注意力加权编码器状态生成，类似LLM生成文本时对上下文的动态依赖。

波束搜索（Beam Search）​​：
LLM生成文本的核心解码方法，通过保留Top-k候选路径平衡质量与多样性。
​​长度归一化​​：避免短句偏好，优化生成结果（如LLM生成文本时对流畅性和信息密度的平衡）。
​​采样策略​​：
文档提到的蒙特卡罗采样（如Temperature Scaling、Top-p采样）是LLM生成多样化文本的常用技术。

​反向翻译（Backtranslation）​​：
利用单语数据生成合成双语语料，提升模型性能。LLM的预训练中也广泛采用类似策略：
​​数据增强​​：通过回译生成多样化训练样本。
​​低资源场景​​：解决平行语料不足问题（如LLM在多语言任务中的泛化能力）。
​​多语言对齐​​：
文档提到的句子对齐和跨语言嵌入（如BERTScore）与LLM的多语言表示学习直接相关。

开始看文档里面提到的第一篇论文，刚开始，看的不多，看到在GS上有一个自带的AI辅助阅读，接下来可以试试它的效果。
### 其他

进行支线任务，在线上试用模型的时候，使用的是deepseek的模型，问它什么是非洲平头哥，它回答说是鸽子的一种,重新进入又问一遍它又说是一种打击乐器，感觉有点胡言乱语，使用其他模型可正常回答。
输入个人API key，正常使用。更改prompt，也可以正常回答。